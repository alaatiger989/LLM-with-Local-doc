{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e6f8b2",
   "metadata": {},
   "source": [
    "# Running model with local docs : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de6fa2",
   "metadata": {},
   "source": [
    "# 1) Constants : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea0fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, Docx2txtLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0845f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder for storing database\n",
    "SOURCE_DIRECTORY = f\"SOURCE_DOCUMENTS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f6bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIRECTORY = f\"db-multilingual-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edc520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be changed to a specific number\n",
    "INGEST_THREADS = os.cpu_count() or 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25265d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(INGEST_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e35160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    is_persistent=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27bf75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GPU_LAYERS = 100  # Llama-2-70B has 83 layers\n",
    "N_BATCH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "039810d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_MAP = {\n",
    "    \".txt\": TextLoader,\n",
    "    \".md\": UnstructuredMarkdownLoader,\n",
    "    \".py\": TextLoader,\n",
    "    \".pdf\": PDFMinerLoader,\n",
    "#     \".pdf\": UnstructuredFileLoader,\n",
    "    \".csv\": CSVLoader,\n",
    "#     \".xls\": UnstructuredExcelLoader,\n",
    "#     \".xlsx\": UnstructuredExcelLoader,\n",
    "    \".docx\": Docx2txtLoader,\n",
    "    \".doc\": Docx2txtLoader,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdc3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\pdf\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f5af7",
   "metadata": {},
   "source": [
    "# Ingest : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified from video\n",
    "def load_single_document(file_path: str) -> Document:\n",
    "    # Loads a single document from a file path\n",
    "    if file_path.endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path , encoding = \"utf8\")\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        loader = PDFMinerLoader(file_path)\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        loader = CSVLoader(file_path)\n",
    "    return loader.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from video\n",
    "def load_documents(source_dir: str) -> list[Document]:\n",
    "    # Loads all documents from the source documents directory, including nested folders\n",
    "    all_files = os.listdir(source_dir)        \n",
    "    return [load_single_document(f\"{source_dir}/{file_path}\") for file_path in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from video\n",
    "def main():\n",
    "    # Load documents and split in chunks\n",
    "    print(f\"Loading documents from {SOURCE_DIRECTORY}\")\n",
    "    documents = load_documents(SOURCE_DIRECTORY)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}\")\n",
    "    print(f\"Split into {len(texts)} chunks of text\")\n",
    "\n",
    "    \n",
    "    # Create embeddings\n",
    "#     EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large\" \n",
    "    \n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large\",\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "    )\n",
    "\n",
    "    db = Chroma.from_documents(\n",
    "        texts,\n",
    "        embeddings,\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        client_settings=CHROMA_SETTINGS,\n",
    "    )\n",
    "    db.persist()\n",
    "    db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc4169",
   "metadata": {},
   "source": [
    "# Run Local GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc0181e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import utils\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ctransformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11312da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    local_llm = GPT4All(\n",
    "        device = 'gpu',\n",
    "        model=\"C:/Users/Alaa AI/Python Projects/Chatbots/GPT by me/alaa_ai_model_gptj.gguf\",\n",
    "        max_tokens=2048,\n",
    "        allow_download=False,\n",
    "        backend=\"mistral\"\n",
    "    #     verbose=True,\n",
    "    )\n",
    "    print(\"Local LLM Loaded\")\n",
    "\n",
    "    return local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa175f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large\" \n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"intfloat/multilingual-e5-large\",\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "    )\n",
    "\n",
    "    db = Chroma(\n",
    "        embedding_function = embeddings,\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        client_settings=CHROMA_SETTINGS,\n",
    "    )\n",
    "    \n",
    "    retriever = db.as_retriever()\n",
    "    # Prepare the LLM\n",
    "#     callbacks = [StreamStdOutCallbackHandler()]\n",
    "    # Load the LLM for generating Natural Language response.\n",
    "    llm = load_model()\n",
    "    print(\"Model is Loaded Successfully ....\")\n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(llm = llm , chain_type = \"stuff\" , retriever = db.as_retriever(search_kwargs = {\"k\" : 3}))\n",
    "    while True:\n",
    "        query = input(\"\\nEnter a query : \")\n",
    "        if query == 'exit':\n",
    "            break\n",
    "            \n",
    "        # Get the answer from the chain\n",
    "        input_text = GoogleTranslator(source = 'auto' , target = 'en').translate(query)\n",
    "        res = qa(input_text)\n",
    "        answer = res['result']\n",
    "        \n",
    "        # Print the result\n",
    "        print(\"\\n\\n> Question : \")\n",
    "        print(query)\n",
    "        print(\"\\n> Answer : \")\n",
    "        print(GoogleTranslator(source = 'auto' , target = 'ar').translate(answer))\n",
    "        \n",
    "#         # Print the relevant sources used for the answer\n",
    "#         print(\"------------------------------- SOURCE DOCUMENTS ----------------------------------\")\n",
    "#         for document in docs:\n",
    "#             print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n",
    "#             print(document.page_content)\n",
    "#         print(\"------------------------------- SOURCE DOCUMENTS ----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ddb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fdae50",
   "metadata": {},
   "source": [
    "# Another Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42aae259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import GPT4All\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967d4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"C:/Users/Alaa AI/Python Projects/Chatbots/GPT with local docs/SOURCE_DOCUMENTS/arabtesting_1_en.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f453e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57a8c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c19391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter  One\n",
      "Scale  for  developmental  \n",
      "delay  and  severe  and  multiple  disabilities\n",
      "1  Machine Translated by Google\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12835703",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b2d9678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7360fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "com.arabtesting.www:// http  \n",
      "com.arabtesting@info  ÿÿÿÿÿÿÿÿÿ:  Machine Translated by Google\n"
     ]
    }
   ],
   "source": [
    "print(texts[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "964a719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b6dd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\pdf\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb7dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db-multilingual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3346013",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee2c9c70-1413-488e-9723-38d212b83e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
    "Read the given context before answering questions and think step by step. If you can not answer a user question based on \n",
    "the provided context, inform the user. Do not use any other information for answering user. Provide a detailed answer to the question.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f39ef6c-273b-4ee1-968c-2a8e38235e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt: str, system_prompt: str = system_prompt) -> str:\n",
    "    return f\"\"\"\n",
    "    [INST] <>\n",
    "    {system_prompt}\n",
    "    <>\n",
    "    \n",
    "    {prompt} [/INST]\n",
    "    \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4edc256-85cd-4cef-8a83-8fbceeff96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"<s>[INST] \", \" [/INST]\"\n",
    "prompt_template = (\n",
    "                B_INST\n",
    "                + system_prompt\n",
    "                + \"\"\"\n",
    "            \n",
    "            Context: {context}\n",
    "            User: {question}\"\"\"\n",
    "                + E_INST\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53349627",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ممكن تكلمنى عن قانون تعليم الأطفال idea 'Respond in details'\"\n",
    "input_text = GoogleTranslator(source = 'auto' , target = 'en').translate(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ef0d96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you talk to me about the Children's Education Law? idea 'Respond in details'\n"
     ]
    }
   ],
   "source": [
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "570c2b56-fb06-403b-b60b-cf9b7b77e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39859804-4d16-4f66-b483-02f3d041d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36ebca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "docs = retriever.get_relevant_documents(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42ebc6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GPT4All(\n",
    "    device = 'gpu',\n",
    "    model=\"C:/Users/Alaa AI/Python Projects/Chatbots/GPT by me/alaa_ai_model_gptj_v1.6.gguf\",\n",
    "    max_tokens=2048,\n",
    "    allow_download=False,\n",
    "    backend=\"mistral\",\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffdaf410",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "  llm, retriever=retriever ,chain_type= \"stuff\", chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "# qa_chain(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c6dce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 51s\n",
      "Wall time: 1min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"Can you talk to me about the Children's Education Law? idea 'Respond in details'\",\n",
       " 'result': ''}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "qa_chain(\n",
    "    input_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adc63a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(GoogleTranslator(source = 'auto' , target = 'ar').translate(res[\"result\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f17daf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"ممكن تكلمنى عن قانون تعليم الأطفال idea 'Respond in details'\",\n",
       " 'result': \"\\n</s>\\n\\nThe Education for Individuals with Difficulties Act 2004, also known as the Improvement of Education for Disabilities with Individuals Act, is a law that focuses on providing education to individuals with difficulties or disabilities. This act covers developmental delay and severe and multiple disabilities. The purpose of this law is to ensure that these children receive an appropriate education tailored to their needs.\\n\\nThe law applies from birth up to the age of 15, and in cases of a severe delay, it may be applied to larger loans. It takes into account that the child's subculture might not encourage long verbal responses, so the focus is on providing an education that caters to their specific needs and abilities.\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain(\"what is the figure in the document? Extract the answer from the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e608fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bdcef-3fa2-4620-9181-870192025337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
